오늘은 머신러닝 4주차의 나머지와 5주차 강의와 알고리즘 코드카타 문제를 풀어보았다.

### **군집화 모델**

---

### **1\. 차원축소 - t-SNE**

1.  정의
    -   t-SNE(t-Distributed Stochastic Neighbor Embedding)는 고고차원 데이터를 저차원으로 변환하여 시각화하는 차원 축소 기법이다.
    -   데이터 포인트 간의 유사성을 보존하면서, 고차원 데이터를 2차춴 또는 3차원 공간으로 변환한다.
    -   데이터 구조와 패턴을 시각적으로 이해할 수 있다.
2.  작동 원리
    1.  고차원 데이터 포인트 간의 유사성을 확률 분포로 게산한다.
    2.  저차원 데이터 포인트 간의 유사성을 t-분포를 사용하여 계산한다.
    3.  고차원 공간과 저차원 공간 간의 유사성 분포 차이를 KL 발상을 통해 최소화한다.
    4.  저차원 공간에서의 데이터 포인트 위치를 반복적으로 조정하여 최적의 시화를 얻는다.
3.  장점
    -   고차원 데이터를 저차원 데이터로 변환하기 때문에 데이터의 구조와 패턴을 쉽게 이해할 수 있고 비선형 구조를 효과적으로 탐지할 수 있다.
    -   데이터의 클러스터를 명확하게 이해할 수 있고 비선형 구조를 효과적으로 탐지할 수 있다.
    -   데이터의 클러스터를 명확하게 시각화 할 수 있다.
    -   고차원의 데이터를 2차원 또는 3차원으로 변환하여 시각화할 수 있다.
4.  단점
    -   계산 비용이 엄청나다.
    -   고차원 데이터의 유사성을 일일이 계산한 뒤, 저차원으로 변환하는 계산이 많이 필요하기 때문에 데이터 셋이 클 경우 시간이 엄청 늘어난다.
    -   매개 변수를 설정해야 한다.
    -   이론적 기법을 코드로 직접 구현 할 때도 학습률 등의 민감한 반응을 보인다.
    -   해석하기가 어렵다.
5.  사용기법
    -   TSNE(n\_components=, random\_state=): 모델을 생성한다.

### **2\. 차원 축소 - LDA**

1.  정의
    -   LDA(선형 판별 분석)는 차원 축소와 분류를 동시에 수행한다.
    -   데이터의 클래스 간 분산을 최대화하고, 클래스 내 분산을 최소화하는 방향으로 데이터를 변환한다.
    -   데이터의 분류 성능을 향상시키고, 저차원 공간에서 데이터의 구조를 시각화할 수 있다.
    -   데이터가 많이 필요하다.
2.  작동 원리
    1.  각 클래스의 평균 벡터를 계산한다.
    2.  클래스 내 데이터 포인트의 분산을 계산하여 클래스 내 분산 행렬을 만든다.
    3.  클래스 간 평균 벡터의 분산을 계산하여 클래스 간 분산 행렬을 만든다.
    4.  클래스 내 분산 행렬의 역행렬과 클래스 간 분산 행렬의 곱의 고유 값과 고유 벡터를 계산한다.
    5.  고유 값이 큰 순서대로 고유 벡터를 정렬하여 선형 판별 축을 선택한다.
    6.  선택된 선형 판별 축을 사용하여 데이터를 저차원 공간으로 변환한다.
3.  장점
    -   차원 축소를 통해 데이터의 주요 특징을 유지하면서 데이터를 저차원 공간에 효율적으로 표현할 수 있다.
    -   클래스 간의 분리를 최대화하기 때문에 분류 문제에서 성능이 향상이 된다.
    -   선형 변환을 사용한다.
4.  단점
    -   선형 변환의 정도에 따라 성능이 떨어질 수 있다.
    -   클래스가 어느 정도 통계적인 특징을 따른다고 가정을 해서 성능이 떨어질 수 있다.
    -   데이터가 뚜렷하게 구분되어 있지 않으면 성능이 많이 저하된다.
5.  사용 기법
    -   LinearDiscriminantAnalysis(n\_components=): 모델을 생성한다.

### **앙상블 학습**

---

### **1\. 정의**

-   여러개의 학습 모델을 결합하여 하나의 강력한 모델을 만드는 기법이다.
-   단일 모델보다 더 높은 예측 성능과 일반화 능력을 얻을 수 있다.

### **주요 기법**

---

### **1\. 배깅**

1.   정의
    -   여러개의 학습 모델을 병렬로 학습시키고, 그 에측 결과를 평균 또는 다수결로 결합하는 앙상블 기법이다.
    -   데이터 샘플링 과정에서 부트스트래핑 기법을 사용해, 원본 데이터 셋에서 중복을 허용한 무작위 샘플을 생성한다.
    -   각 모델은 서로 다른 데이터 샘플을 학습하게 되어, 모델 간의 상관성을 줄이고 예측 성능을 향상한다.
2.  장점
    -   여러 모델의 예측을 결합함으로써 과적합을 줄일 수 있다.
    -   데이터의 변동에 덜 민감해진다.
    -   각 모델을 독립적으로 학습 시킬 수 있어 병럴 처리가 가능하다.
3.  단점
    -   계산 비용이 증가한다.
    -   여러 학습기를 학습시키고 추론도 여러번 하여 상대적으로 속도가 느리다.
    -   여러개를 복합적으로 계산해야하기 때문에 해석하기가 어렵다.
4.  사용 기법
    -   BaggingRegressor(estimator= 복합적으로 만들 모델, n\_estimators=평가 개수의 수): 모델을 생성한다.

### **2.부스팅**

1.  정의
    -   여러 개의 약한 학습기를 순차적으로 학습시키고, 그 예측 결과를 결합하여 강한 학습기를 만든다.
    -   이전 모델이 잘못 예측한 데이터 포인트에 가중치를 부여하여, 다음 모델이 이를 더 잘 학습하도록 한다.
2.  작동 원리
    1.  모든 데이터에 동일한 가중치를 할당해서 가중치를 일반화 시킨다.
    2.  학습기를 순차적으로 학습 시키게 되는데 첫번째 학습기를 학습시키고 학습기의 오차를 제거한다.
    3.  잘못 분류된 데이터들에게 더 큰 가중치를 부여하고 학습기가 더 잘 학습하도록 유도한다.
    4.  학습기가 학습을 완료할 때마다, 잘 못 분류된 데이터 포인트의 가중치가 점점 증가하게 되고 잘 분류된 데이터는 더 이상 학습을 진행하지 않는다.
    5.  이를 통해 순차적으로 나열된 학습기가 점점 어려운 데이터 포인트에 집중하게 한다.
3.  장점
    -   약한 학습기를 결합하여 높은 예측 성능을 얻을 수 있다.
    -   모델의 복잡도를 조절하여 과적합을 방지할 수 있다.
    -   이전 모델의 오류를 보완하는 방식으로 학습이 진행된다.
4.  단점
    -   여러 개의 약한 학습기를 순차적으로 학습시키기 때문에 시간과 자원이 많이 소모될 수 있다. 특히, 데이터셋이 크거나 복잡할 때 훈련 시간이 길어질 수 있다.
    -   데이터에 노이즈(오류나 비정상적인 데이터)가 있을 경우, Boosting은 이를 과도하게 학습할 수 있다. 이는 모델이 노이즈를 일반 패턴으로 인식하여 성능을 떨어뜨릴 수 있다.
    -   Boosting은 순차적으로 학습기를 학습시키는 방식이라 병렬 처리나 분산 처리에 적합하지 않다. 각 단계의 결과에 따라 다음 모델이 학습되기 때문에 병렬화가 어려운 구조이다.
5.  사용 기법
    -   GradientBoostingRegressor(): 모델을 생성한다.

### **3\. 랜덤 포레스트**

1.  정의
    -   배깅 기법을 기반으로 한 앙상블 학습 모델이다.
    -   여러 개의 결정 트리를 학습시키고, 예측 결과를 결합하여 최종 예측을 수행한다.
    -   각 트리가 독립적으로 학습되기 때문에, 과적합 방지와 예측 성능을 향상시킨다.
2.  구조
    -   여러 개의 결정 트리로 구성된다.
    -   각 결정 트리는 데이터의 무작위 샘플을 사용하여 학습되며, 트리의 에측 결과를 평균 또는 다수결에 결합하여 최종 예측을 수행한다.
3.  작동 원리
    -   원본 데이터 셋에서 중복을 허용한 무작위 샘플을 생성한다.
    -   각 부트스트랩 샘플을 사용하여 결정 트리를 학습시킨다. 이때, 각 노드에서 무작위로 선택된 특성의 일부만을 사용하여 분할을 수행한다.
    -   모든 결정 트리의 예측 결과를 결합하여 최종 에측을 수행합니다. 회귀 문제에서는 평균을 사용하고, 분류 문제에서는 다수결을 사용한다.
4.  장점
    -   해석이 쉽고 특징을 무작위로 사용하여서 특정 모델들이 조금 다형성이 증가한다.
    -   변수에 중요도를 평가할 수 있다.
5.  단점
    -   여러 개의 트리를 학습시키고 예측을 위해 각 트리의 결과를 결합해야 하므로 훈련 시간과 예측 시간이 비교적 오래 걸린다. 특히 데이터셋이 크거나, 트리 개수가 많을수록 계산 비용이 증가한다.
    -   일반적으로 과적합에 강한 모델이지만, 너무 많은 트리를 사용하거나 특정 하이퍼파라미터 설정에 따라 여전히 과적합이 발생할 수 있다. 특히 데이터에 노이즈가 많을 경우 트리가 불필요하게 복잡해질 수 있다.
    -   많은 수의 결정 트리를 동시에 저장하고 사용해야 하기 때문에, 메모리 사용량이 많아질 수 있다. 특히 데이터셋이 클수록 메모리 부담이 커진다.
    -   범주형 변수를 처리할 수는 있지만, 많은 범주를 가진 변수에 대해서는 잘 작동하지 않을 수 있다.
6.  사용 기법
    -   RandomForestRegressor(): 모델을 생성한다.

### **4\. 그래디언트 부스팅 머신**

1.  정의
    -   그래디언트 부스팅 머신 (Gradient Boosting Machine, GBM)은 여러 개의 약한 학습기를 순차적으로 학습시키고, 그 에측 결과를 결합하여 강한 학습기를 만드는 앙상블 기법이다.
    -   이전 모델이 잘못 예측한 데이터 포인트에 가중치를 부여하여, 다음 모델이 이를 더 잘 학습하도록 한다.
    -   각 트리가 독립적으로 학습되기 때문에, 과적합을 방지하고 예측 성능을 향상시킬 수 있다.
2.  구조
    -   여러 개의 결정 트리로 구성된다.
    -   각 결정 트리는 이전 트리의 예측 오류를 보완하는 방식으로 학습 된다.
    -   각 트리의 예측 결과를 가중합하여 최종 예측을 수행한다.
3.  작동 원리
    1.  첫 번째 결정 트리를 학습시켜 초기 모델을 만든다.
    2.  초기 모델의 예측 결과와 실제 값 간의 잔여 오차를 계산한다.
    3.  잔여 오차를 예측하는 새로운 결정 트리를 학습시킨다.
    4.  새로운 결정 트리를 기존 모델에 추가하여 모델을 업데이트한다.
    5.  잔여 오차가 충분히 작아질 때까지 2~4단계들을 반복한다.
4.  장점
    -   여러 약한 학습기를 결합해 매우 강력한 예측 성능을 발휘한다. 특히, 복잡한 비선형 데이터나 고차원 데이터에서도 좋은 성능을 보인다.
    -   회귀(regression), 분류(classification), 순위 매기기(ranking) 등 다양한 문제에 적용할 수 있다. 또한, 다양한 손실 함수(loss function)를 사용하여 문제의 특성에 맞게 최적화할 수 있다.
    -   과적합을 방지할 수 있는 다양한 정규화 기법(예: 학습률 조정, 트리 깊이 제한)을 지원한다.
    -   모델에서 어떤 특성이 예측에 중요한지 알 수 있는 변수 중요도 정보를 제공한다. 이를 통해 모델의 의사결정에 중요한 변수를 파악할 수 있다.
    -   부스팅 기법의 특성상 데이터의 오류나 노이즈에 비교적 강하게 대응할 수 있다.
5.  단점
    -   트리를 하나씩 순차적으로 학습시키기 때문에, 훈련 시간이 길어질 수 있다. 특히, 큰 데이터셋에서는 속도가 문제가 될 수 있다.
    -   각 트리가 이전 트리의 오차를 기반으로 학습하므로, 계산 자원이 많이 소모된다. 많은 데이터와 트리가 필요할 경우, 메모리 사용량과 계산 비용이 크게 증가한다.
    -   트리를 순차적으로 학습하므로 병렬 처리가 어렵다. 이로 인해 학습 속도가 느릴 수 있다.
6.  사용 기법
    -   GradientBoostingRegressor(): 모델을 생성한다.

### **5\. XGBoost**

1.  정의
    -   그래디언트 부스팅 알고리즘을 기반으로 한 고성능 앙상블 학습 기법입니다.
    -   효율성, 유연성, 이식성을 목표로 설계되었으며, 다양한 머신러닝 경진대회에서 우수한 성능을 보여주고 있다.
    -   병렬 처리, 조기 종료, 정규화 등의 기능을 통해 성능을 극대화한다.
2.  구조
    -   여러 개의 결정 트리로 구성된다.
    -   각 결정 트리는 이전 트리의 예측 오류를 보완하는 방식으로 학습된다.
    -   각 트리의 예측 결과를 가중합하여 최종 에측을 수행한다.
3.  작동 원리
    1.  첫 번째 결정 트리를 학습시켜 초기 모델을 만든다.
    2.  초기 모델의 예측 결과와 실제 값 간의 잔여 오차를 계산한다.
    3.  잔여 오차를 예측하는 새로운 결정 트리를 학습시킨다.
    4.  새로운 결정 트리를 기존 모델에 추가하여 모델을 업데이트한다.
    5.  잔여 오차가 충분히 작아질 때까지 2 ~ 4단계를 반복한다.
4.  장점
    -   트리의 분할을 병렬로 수행하여 학습 속도를 향상시킨다.
    -   검증 데이터 셋의 성능이 향상되지 않으면 학습을 조기에 종료하여 과적합을 방지한다.
    -   L1 및 L2 정규화를 통해 모델의 복잡도를 조절하고 과적합을 방지한다.
    -   다양한 손실 함수와 평가 지표를 지원하여 다양한 문제에 적용할 수 있다.
5.  단점
    -   하이퍼파라미터 튜닝의 복잡성과 계산 자원의 소모가 크며, 훈련 시간이 길어질 수 있다.
    -   노이즈에 민감하고, 예측 결과를 해석하기 어려운 설명력 부족 문제도 존재한다.
    -   범주형 변수 처리에서 추가적인 전처리가 필요하다.
6.  사용 기법
    -   xgb.XGBRegressor(): 모델을 생성한다.