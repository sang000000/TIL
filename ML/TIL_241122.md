오늘은 LLM 특강을 복습하고 알고리즘 코드카타를 풀어보았다.

### **LLM의 사이즈를 줄이기 위한 방법**

---

### **1\. 양자화**

-   기존에 존재하는 LLM을 압축하기
-   float이 너무 크니까 8bit, 4bit 정수형으로 파라미터를 표현한다.
-   기존의 표현하던 영역을 표현하지 못하게 되어 성능은 낮아진다.
-   얼추 성능이 유사하다고 볼 수 있지만 양자화가 많이 될 수록 성능이 낮아진다.

### **2.SLLM/ SLM**

-   소형 언어 모델이다.
-   small large langauge model이라고 부른다.
-   작은 용량만큼 아직은 성능이 작다.
-   챗봇으로 바로 쓰이기 보다는 특수 목적에 따라서 튜닝해서 사용하는 경우가 많다. 

### **ollama**

---

### **1\. 특징**

-   간단하고 직관적이며, windows, max, linux 모두 지원한다.
-   심플하고 CLI 위주라서 linux 등 서버에 배포하기도 좋다.
-   올라와 있는 모델들이 기본적으로 모두 4비트 양자화가 되어 있고 hugging face에 올라와 있는 커스텀 모델을 사용할 수 없다는 점이 있다.
-   VRam으로 불러와서 실행하여 터미널 창에서 사용이 가능하다.
-   인터넷 연결이 없어도 가능하다.

### **RAG**

---

### **1\. 개념**

-   일반적인 LLM은 실제로 존재하지 않는 정보를 지어내고 학습된 지식이 아니면 답변이 어려우며, 정확한 답변을 얻으려면 정보를 같이 제공해야 한다.
-   이러한 단점을 보완하기 위해 문서를 저장해두고 물어보면 문서를 검색하고 검색 결과를 같이 챗 쥐피티에 보내서 답변을 생성한다.

### **2\. 실행 단계**

1.  문서 저장 단계
    -   문서를 쪼개고, 임베딩으로 변환해서 저장한다.
    -   임베딩은 텍스트, 이미지 등의 데이터를 기계가 읽을 수 있는 벡터 값으로 바꿔주는 과정, 혹은 바뀐 벡터 값이다.
    -   임베딩 모델을 이용해서 split 된 문서를 임베딩으로 변환한다.
    -   문서의 임베딩을 벡터 데이터 베이스에 넣는다.
2.  질의 응답 단계
    -   질문을 받는다.
    -   벡터 데이터 베이스에서 거리가 가까운 문서들을 조회한다.
    -   가져온 문서를 Prompt에 넣어서 LLM에 물어본다.
    -   답변이 나온다.
