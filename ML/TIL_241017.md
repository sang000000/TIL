오늘은 딥 러닝 1주차와 2주차 강의와 알고리즘 코드카타 문제를 풀어보았다.

### **딥러닝**

---

### **1\. 개념**

-   인공 신경망을 기반으로 한 기계 학습의 한 분야이다.
-   다층 신경망을 사용하여 데이터로부터 특징을 자동으로 학습하고 이를 통해 복잡한 문제를 해결한다.
-   입력 데이터에서 중요한 패턴을 추출하고, 이를 바탕으로 예측, 분류, 생성 등의 다양한 작업을 수행한다.

### **2\. 특징**

-   비선형 추론을 통해 복잡한 데이터의 패턴을 학습할 수 있다.
-   여러 층의 신경망을 사용하여 데이터의 고차원 특징을 학습할 수 있다.
-   데이터로부터 중요한 특징을 자동으로 추출하여 별도의 특징 공학 과정이 필요없다.

### **3\. 활용 방안**

-   이미지 분류, 객체 검출, 이미지 생성 등 다양한 이미지를 처리 작업에 활용한다.
-   번역, 요약, 감정 분석 등 자연어 처리 작업에 사용된다.
-   음성 인식 시스템에 사용된다.
-   의료 영상 분석, 질병 예측, 신약 개발 등 다양한 의료 분야에서도 활용 된다.

### **4\. 배워야 하는 이유**

-   성능이 엄청 좋다.
-   여러 분야에 적용할 수 있다.
-   생각 보다 비싸지 않지만 성능은 엄청 좋다.
-   입력에 대한 제한이 줄어들어 일반인들도 사용하기가 좋다.

### **신경망의 기본 원리**

---

### **1\. 퍼셉트론과 다층 퍼셉트론**

1.   단일 퍼셉트론의 개념
    -   퍼셉트론은 인공 신경망의 가장 기본적인 단위로, 하나아ㅢ 뉴런을 모델링한 것이다.
    -   퍼셉트론은 입력 값을 받아 가중치를 곱하고, 이를 모두 더한 후 활성화 함수를 통해 출력 값을 결정한다.
2.  다층 퍼셉트론(MLP)의 개념
    -   다층 퍼셉트론은 여러층의 퍼셉트론을 쌓아 올린 신경망 구조이다.
    -   MLP는 입력층, 은닉층, 출력층으로 구성되며, 각 층의 뉴런들이 서로 연결되어 있다.
3.  입력, 은닉, 출력 레이어의 개념
    -   입력 레이어는 외부 데이터가 신경망에 입력되는 부분이며, 입력 레이어의 뉴런수는 입력 데이터의 특징의 수와 동일하다.
    -   은닉 레이어는 입력 레이어와 출력 레이어 사이에 위치한 층으로, 입력 데이터를 처리하고 특징을 추출하는 역할을 하며, 은닉 레이어의 뉴런 수와 층 수는 모델의 복잡성과 성능에 영향을 미친다.
    -   출력 레이어는 신경망의 마지막 층으로, 최종 예측 값을 출력하며, 출력 레이어의 뉴런 수는 에측하려는 클래스 수 또는 회귀 문제의 출력 차원과 동일하다.
4.  XOR 문제와 MLP
    -   단일 퍼셉트론은 선형 분류기이기 떄문에 XOR 문제와 같은 비선형 문제를 해결하지 못한다.
    -   XOR 문제는 두 입력 값이 다를 때만 1을 출력하는 문제로, 단일 퍼셉트론으로는 해결하지 못한다.
    -   MLP는 은닉층을 통해 비선형성을 학습할 수 있어 XOR 문제를 해결할 수 있다.

### **2\. 활성화 함수**

1.  필요성
    -   활성화 함수는 신경망의 각 뉴런에서 입력 값을 출력 값으로 변환하는 역할을 한다.
    -   활성화 함수가 없다면 신경망은 단순 선형 변환만 수행하게 되어 복잡한 패턴을 학습할 수 없다.
    -   비선형성을 도입하여 신경망이 복잡한 패턴을 학습할 수 있게 한다.
2.  종류
    -   Relu (Rectified Linear Unit )
        -   계산이 간단하고 기울기 소실 문제를 온화한다.
        -   음수 입력에 대해 기울기가 0이 되는 죽은 Relu 문제가 발생한다.
    -   Sigmoid
        -   출력 값이 0과 1사이로 제한되어 확률 표현하기에 적합하다.
        -   기울기 소실 문제와 출력 값이 0 또는 1에 가까워질 때 학습이 느려지는 문제가 생긴다.
    -   Tanh (Hyperbolic Tangent)
        -   출력 값이 -1과 1 사이로 제한되어 중심이 0에 가까워진다.
        -   기울기 소실 문제가 발생한다.

### **3\. 손실 함수**

1.  역할
    -   손실 함수는 모델의 예측 값과 실제 값 사이의 차이를 측정하는 함수이다.
    -   손실 함수는 모델의 성능을 평가하고, 최적화 알고리즘을 통해 모델을 학습 시키는데 사용한다.
2.  종류
    -   MSE (Mean Squared Error)
        -   회귀 문제에서 주로 사용된다.
        -   예측 값과 실제 값의 차이를 제곱하여 평균을 구한다.
    -   Cross-Entropy
        -   분류 문제에서 주로 사용된다.
        -   예측 확률과 실제 클래스 간의 차이를 측정한다.

### **4\. 최적화 알고리즘**

1.  개념
    -   최적화 알고리즘은 손실 함수를 최소화 하기 위해 모델의 가중치를 조정하는 방법이다.
    -   손실 함수의 기울기를 계산하고, 이를 바탕으로 가중치를 업데이트한다.

### **5\. 역전파 알고리즘**

1.  개념
    -   역전파는 신경망의 가중치를 학습시키기 위해 사용되는 알고리즘이다.
    -   출력에서 입력 방향으로 손실 함수의 기울기를 계산하고 이를 바탕으로 가중치를 업데이트 한다.
2.  원리
    -   연쇄 법칙을 사용해 손실 함수의 기울기 계산한다.
    -   각 층의 기울기는 이전 층의 기울기와 현재 층의 기울기를 곱하여 계산한다.
    -   이를 통해 신경망의 모든 가중치가 업데이트 된다.

### **인공 신경망 (ANN)**

---

### **1\. 구성요소**

-   생물학적 신경망을 모방하여 설게된 컴퓨팅 시스템이다.
-   입력층(Input Layer), 은닉층(Hidden Layer), 출력층(Output Layer)으로 구성되며, 각 층은 뉴런으로 이루어져 있다.
-   입력층은 입력 데이터를 받아들이는 층으로 입력층의 뉴런수는 입력 데이터 피쳐수와 동일하다.
-   은닉층은 입력 데이터를 처리하고 특징을 추출하는 층으로 은닉층의 뉴런 수와 층수는 모델의 복잡성과 성능에 영향을 미치며, 연산을 하는데 가장 중요한 부분이다.
-   출력층은 최종 예측 값을 출력하는 층으로 출력층의 뉴런 수는 에측하려는 클래스 수 또는 회귀 문제 출력 차원과 동일하다.

### **2\. 동작 방법**

1.  입력 데이터를 통해 각 층의 뉴런이 활성화되고, 최종 출력 값을 계산한다.
2.  예측 값과 실제 값의 차이를 손실 함수로 계산한다.
3.  손실 함수의 기울기를 출력 층에서 입력 층으로 계산하고, 이를 바탕으로 가중치를 업데이트 한다.

### **3\. 출력 레이어의 유형과 활용**

1.  회귀 문제(Regression)
    -   출력 레이어의 뉴런 수는 예측하려는 연속적인 값의 차원과 동일하다.
    -   활성화 함수로는 주로 선형 함수를 사용한다.
2.  이진 분류 문제(Binary Classification)
    -   출력 레이어의 뉴런 수는 1이다.
    -   활성화 함수로는 시그모이드 함수를 사용하여 출력 값을 0과 1 사이의 확률로 변환한다.
3.  다중 클래스 분류 문제(Multi-Class Classification)
    -   출력 레이어의 뉴런 수는 예측하려는 클래스 수와 동일하다.
    -   활성화 함수로는 소프트맥스 함수를 사용하여 각 클래스에 대한 확률을 출력한다.

### 4\. 사용 기법

-   torch: 핵심 라이브러리로 여러 기능들을 포함한다.
-   torch.nn: 신경망 구축을 위한 기능들을 포함한다.
-   torch.optim: 최적화 기능을 포함한다.
-   torchvision: 이미지 처리를 하기 위한 라이브러리이다.
-   torchvision.transform: 전처리를 위한 라이브러리이다.
-   transforms.ToTensor(): 이미지를 Pytorch에서 사용하는 기본 자료구조로 바꿔준다.
-   transforms.Normalize(표준 편차, 평균): 이미지를 정규화 해준다.
-   batch\_size: 데이터를 잘게 쪼개서 빠르게 학습이 될 수 있게 한다.
-   shuffle : 동일성을 보장해주기 위해 데이터를 섞어서 배치를 만들게 한다.
-   torch.relu(): 활성화 해준다.

### **합성곱 신경망(CNN)**

---

### **1\. 기본 구조**

-   이미지와 같은 2차원 데이터의 특징을 효과적으로 추출하기 위해 설계된 신경망이다.
-   합성곱 층, 풀링 층, 완전 연결 층으로 연결된다.
-   합성곱 층은 입력 이미지에 필터(커널)를 적용해 특징 맵을 생성하며,.필터는 이미지의 국소적인 패턴을 학습한다.
-   풀링 층은 특징 맵의 크기를 줄이고, 중요한 특징을 호출하며, 주로 Max Pooling과 Average Pooling이 사용된다.
-   완전 연결 층은 추출된 특징을 바탕으로 최종 예측을 수행하며, CNN이라는 분석 레이어를 통해 추출한 특성을 바탕으로 결론을 내리는 부분이다.

### **2\. 합성곱 연산의 원리와 필터의 역할**

1.  합성곱 연산
    -   필터를 이미지의 각 위치에 슬라이딩하며, 필터와 이미지의 해당 부분 간의 점곱을 계산한다.
    -   계산된 값은 특징 맵의 해당 위치에 저장됩니다.
2.  필터의 역할
    -   필터는 이미지의 에지(edge), 코너(corner), 텍스처(texture) 등 다양한 국소적인 패턴을 학습한다.
    -   여러 개의 필터를 사용하여 다양한 특징 맵을 생성할 수 있다.

### **3\. 풀링 레이어의 필요성과 종류**

1.  Max Pooling
    -   필터 크기 내에서 최대 값을 선택한다.
    -   중요한 특징을 강조하고, 불필요한 정보를 제거한다.
2.  Average Pooling
    -   필터 크기 내에서 평균 값을 계산한다.
    -   특징 맵의 크기를 줄이면서, 정보의 손실을 최소화한다

### **4.****플래튼 레이어의 역할**

-   플래튼 층(Flatten Layer)은 2차원 특징 맵을 1차원 벡터로 변환하는 역할을 한다. 이는 완전 연결 층에 입력으로 사용하기 위해 필요하다.

### **순환 신경망(RNN)**

---

### **1\. 기본 구조**

-   시계열 데이터나 순차적인 데이터를 처리하기 위해 설계된 신경망이다
-   RNN은 이전 시간 단계의 정보를 현재 시간 단계로 전달해, 시퀀스 데이터의 패턴을 학습할 수 있다.

### **2\. RNN의 동작 방법순환 구조**

-   RNN은 입력 데이터와 이전 시간 단계의 은닉 상태를 입력으로 받아, 현재 시간 단계의 은닉 상태를 출력한다.
-   은닉 상태는 시퀀스의 정보를 저장하고, 다음 시간 단계로 전달된다.
-   RNN은 시퀀스의 각 시간 단계에서 동일한 가중치를 공유하여, 시퀀스의 패턴을 학습한다.
-   순전파(Forward Propagation)와 역전파(Backpropagation Through Time, BPTT)를 통해 가중치를 학습한다.

### **3\. LSTM & GRU**

-   RNN은 장기 의존성 문제를 겪을 수 있다. 이를 해결하기 위해 LSTM과 GRU가 개발되었다.

###  **4. LSTM(Long Short-Term Memory)**

-   셀 상태와 게이트 구조를 도입, 장기 의존성을 효과적으로 학습가능 한다.
-   입력 게이트, 출력 게이트, 망각 게이트를 사용하여 정보를 조절한다.

### **5.GRU (Gated Recurrent Unit)**

-   LSTM의 변형으로, 셀 상태 대신 은닉 상태만을 사용하여 구조를 단순화한다.
-   업데이트 게이트와 리셋 게이트를 사용하여 정보를 조절한다.

### **6\. 차이점**

-   LSTM은 셀 상태와 은닉 상태를 모두 사용하며, 더 복잡한 게이트 구조를 가진다.
-   GRU는 은닉 상태만을 사용하며, 더 간단한 게이트 구조를 가진다. 따라서 계산 비용이 적고, 학습이 빠를 수 있다.