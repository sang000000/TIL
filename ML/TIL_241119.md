오늘은 개인 과제(도전 과제)를 완성하였고 알고리즘 코드카타와 복습을 해보았다.

### **개인 과제**

---

### **도전 과제 2번 문제**

1) 문제 설명

-   각 프롬프트를 외부에서 불러와서 실행할 수 있도록 코드를 고쳐라.

### **도전 과제 2번 완성 코드**

#도전 과제 2. 각 프롬프트를 외부에서 불러와서 실행할 수 있도록 코드를 고쳐라.

from langchain.prompts import ChatPromptTemplate

from langchain.chains import LLMChain

import os

import time

  

\# 파일로 저장한 프롬프트를 가져옴

while True:

    try:

        a \= input("가져올 프롬프트 이름: ")

        file\_path \= f"C:\\\\Users\\\\h\\\\Desktop\\\\TIL\\\\personal\_assignment\\\\Prompts\\\\{a}" \# 파일 경로

        with open(file\_path, "r") as f:

            prompt \= f.read() \# 파일을 읽어서 저장

        break

    except FileNotFoundError and OSError:

        print("없는 파일입니다!")

  

\# 프롬프트 템플릿 생성

prompt\_template \= ChatPromptTemplate.from\_template(prompt)

#텍스트 생성 체인 설정

llm\_chain \= LLMChain(llm\=model, prompt\=prompt\_template)

  
  

#챗봇 구동

while True:

    print("========================")

    query \= input("질문을 입력하세요 : ")

    if query \== "종료": \# 종료 입력 시 챗봇 종료

        break

    \# 1. Retriever로 관련 문서 검색

    response\_docs \= rag\_chain\_debug\["context"\].invoke({"question": query})

    \# 2. 문서를 프롬프트로 변환

    prompt\_messages \= rag\_chain\_debug\["prompt"\].invoke({

        "context": response\_docs,

        "question": query

    })

    \# 3. LLM으로 응답 생성

    response \= rag\_chain\_debug\["llm"\].invoke(prompt\_messages)

    print("\\n답변:")

    print(response.content) \# 답변 출력

### **도전 과제 2번 코드 설명**

from langchain.prompts import ChatPromptTemplate

from langchain.chains import LLMChain

import os

import time

  

\# 파일로 저장한 프롬프트를 가져옴

while True:

    try:

        a \= input("가져올 프롬프트 이름: ")

        file\_path \= f"C:\\\\Users\\\\h\\\\Desktop\\\\TIL\\\\personal\_assignment\\\\Prompts\\\\{a}" \# 파일 경로

        with open(file\_path, "r") as f:

            prompt \= f.read() \# 파일을 읽어서 저장

        break

    except FileNotFoundError and OSError:

        print("없는 파일입니다!")

-   While True를 사용하여 무한 루프를 시작한다.
-   try가 실행되어 try 안에 있는 코드들이 실행이 된다.
-   a에 input을 사용하여 가져올 txt 파일 이름을 받아 저장한다.
-   파일의 전체 경로를 찾기 위해 file\_path에 입력 받은 파일에 이름을 a 위치에 넣어 저장한다.
-   with open()을 사용하여 file\_path 경로에 있는 파일을 "r"을 사용하여 읽기 모드로 연다.
-   prompt에는 해당 파일을 read를 사용하여 읽고 저장한다.
-   이 과정까지 오류가 없었다면 break문이 실행되서 반복문이 종료가 될 것이다.
-    해당 위치에 그런 파일이 없어 FileNotFoundError 에러 혹은 OSError가 뜬다면 없는 파일이라는 문구가 뜨고 다시 try로 돌아가고 반복문은 종료가 되지 않는다.

\# 프롬프트 템플릿 생성

prompt\_template \= ChatPromptTemplate.from\_template(prompt)

#텍스트 생성 체인 설정

llm\_chain \= LLMChain(llm\=model, prompt\=prompt\_template)

-   from\_template 메서드는 외부에서 가져와 로컬 파일에 저장한 문자열 형태의 프롬프트를 템플릿 형식으로 변환하고 나중에 사용자 쿼리를 포함하여 LLM에게 전달되는 메시지를 구성하는 데 사용된다.
-   LLMChain 클래스의 인스턴스를 생성하여 텍스트 생성 체인을 설정한다. llm 매개변수로는 사용할 모델을 전달하고 prompt 매개변수로는 앞서 생성한 프롬프트 템플릿을 전달한다.
-   이 체인은 주어진 프롬프트와 모델을 사용하여 입력된 쿼리에 대한 응답을 생성하는 역할을 한다.

while True:

    print("========================")

    query \= input("질문을 입력하세요 : ")

    if query \== "종료": \# 종료 입력 시 챗봇 종료

        break

-   무한 루프를 사용하여 계속 입력을 받아 query에 저장한다.
-   만약 query가 "종료"라면, 즉 종료를 입력하면 무한 루프가 중단된다.

\# 1. Retriever로 관련 문서 검색

    response\_docs \= rag\_chain\_debug\["context"\].invoke({"question": query})

    \# 2. 문서를 프롬프트로 변환

    prompt\_messages \= rag\_chain\_debug\["prompt"\].invoke({

        "context": response\_docs,

        "question": query

    })

    \# 3. LLM으로 응답 생성

    response \= rag\_chain\_debug\["llm"\].invoke(prompt\_messages)

    print("\\n답변:")

    print(response.content) \# 답변 출력

-   rag\_chain\_debug의 "context" 부분을 사용하여 입력된 질문과 관련된 문서를 검색하고 invoke 메서드는 질문을 기반으로 관련 문서를 반환한다.
-   검색된 문서를 사용하여 프롬프트 메시지를 생성하고 이 단계에서 질문과 함께 문서 컨텍스트를 포함하여 LLM이 이해할 수 있는 형식으로 변환한다.
-   최종적으로, 변환된 프롬프트 메시지를 LLM에 전달하여 응답을 생성한다.
-   그 후 출력문을 통해 답변을 출력한다.

### **도전 과제 3번 문제**

1) 문제 설명

-   실행 결과는 자동으로 Result 디렉토리에 저장되어야 한다. 이때, 실험 결과 파일 이름은 실험에 쓰인 프롬프트의 이름과 timestamp을 포함해야한다.

### **도전 과제 1번 완성 코드**

\# 도전 과제 3번 실행 결과는 자동으로 Result 디렉토리에 저장되어야 한다. 이때, 실험 결과 파일 이름은 실험에 쓰인 프롬프트의 이름과 timestamp을 포함해야한다.

    \# 현재 타임스탬프 기록

    timestamp \= time.time()

  

    \# 폴더 및 파일 이름 설정

    folder\_name \= 'Results'

    file\_name \= f"{a\[:7\]}\_result\_{timestamp}.txt"

  

    \# 폴더 생성

    os.makedirs(folder\_name, exist\_ok\=True)

  

    \# 답변 파일 생성

    with open(os.path.join(folder\_name, file\_name), 'w', encoding\='utf-8') as f:

        f.write(response.content)

### **도전 과제 3번 코드 설명**

-   time.time()을 사용하여 현재 시간을 초 단위로 타임스탬프 형식으로 기록한다.
-   folder\_name에 폴더 이름을 저장한다.
-   file\_name에 파일 이름을 저장한다.
-   os.makedirs를 사용하여 'Results' 폴더를 생성한다.
-   with open() 구문을 사용하여 os.path.join()을 사용하여 경로를 지정하고 'W'를 사용하여 쓰기 모드로 진입한 후 답변의 내용을 파일로 저장한다.

### **필수 과제와 차이점**

[##_Image|kage@bxQipK/btsKN6hweFz/krQSxj09gKEFabJfBYfIq0/img.png|CDM|1.3|{"originWidth":1415,"originHeight":243,"style":"alignCenter","caption":"필수 과제","filename":"기존.PNG"}_##]

[##_Image|kage@kFwmN/btsKPy4Y4hv/1IKRFPRf7NpMRHSgSHjRak/img.png|CDM|1.3|{"originWidth":1459,"originHeight":328,"style":"alignCenter","caption":"도전 과제","filename":"새로운.PNG"}_##]

-   필수 과제에서는 주어진 컨텍스트에 명확히 제한된 상태에서 질문에 대한 답변을 생성하도록 설계되있다.
-   즉, 모델은 주어진 컨텍스트만을 바탕으로 응답을 생성해야 하므로, 더 구체적이고 관련성 높은 답변을 제공한다.
-   도전 과제는 특정한 목적에 맞춰 사전 학습된 모델을 사용하여 질문에 대한 답변을 생성한다. 따라서, 모델이 훈련된 데이터에 기반하여 일반적인 패턴을 따르는 응답을 생성한다.