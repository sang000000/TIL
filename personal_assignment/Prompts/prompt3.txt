input_variables=['input', 'output'] input_types={} partial_variables={} metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-answer-hallucination', 'lc_hub_commit_hash': 'a88d01cb864e906293aae38575a85627b3b932d6ba2210de4bd69cea9bfd99ab'} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \n\nGive a binary score 1 or 0, where 1 means that the answer is grounded in / supported by the set of facts.', template_format='mustache'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={}, template='Facts: {{input.documents}} \n\nLLM generation: {{output}}', template_format='mustache'), additional_kwargs={})] schema_={'type': 'object', 'title': 'Criteria', 'required': ['Score', 'Explanation'], 'properties': {'Score': {'type': 'integer', 'description': 'Is the LLM generation grounded in the Facts?'}, 'Explanation': {'type': 'string', 'description': 'Explain your reasoning for the score:'}}, 'description': 'Score the LLM generation for whether it is grounded in the Facts.'} structured_output_kwargs={}