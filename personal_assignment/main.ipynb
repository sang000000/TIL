{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. 사용환경 준비\n",
    "import os\n",
    "from getpass import getpass\n",
    "import requests\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass(\"OpenAI API key 입력: \") # API 키 입력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. 모델 로드하기 \n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# 모델 초기화\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': '인공지능산업최신동향_2024년11월호.pdf', 'page': 0}, page_content='2024년 11월호')]\n"
     ]
    }
   ],
   "source": [
    "#3. 문서 로드하기\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# PDF 파일 로드. 파일의 경로 입력\n",
    "loader = PyPDFLoader(\"인공지능산업최신동향_2024년11월호.pdf\")\n",
    "\n",
    "# 페이지 별 문서 로드\n",
    "docs = loader.load()\n",
    "print(docs[:1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': '인공지능산업최신동향_2024년11월호.pdf', 'page': 0}, page_content='2024년 11월호')]\n"
     ]
    }
   ],
   "source": [
    "#4. 문서 청크로 나누기(CharacterTextSplitter)\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# 텍스트 청크 분할기 설정 (문단 기준 분할)\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\\n\",\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=10,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents(docs) # 문서를 청크로 분할\n",
    "print(splits[:1]) # 상위 10개만 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CharacterTextSplitter 청킹 방식 및 매개변수 설명\n",
    "\n",
    "## 청킹 방식\n",
    "`CharacterTextSplitter`는 텍스트를 지정된 기준에 따라 나누어 작은 조각(청크)으로 만드는 기능을 제공한다. 이 방식은 대량의 텍스트를 처리할 때 유용합니다.\n",
    "\n",
    "## 매개변수 설명\n",
    "\n",
    "1. **separator**: \n",
    "   - **설명**: 문서를 나눌 기준으로 사용할 문자열을 설정한다는 뜻이다.\n",
    "\n",
    "2. **chunk_size**:\n",
    "   - **설명**: 각 청크의 최대 크기를 설정한다. 청크는 이 크기를 초과하지 않는다.\n",
    "\n",
    "3. **chunk_overlap**:\n",
    "   - **설명**: 청크 간의 겹치는 글자 수를 설정한다. 문맥을 유지하는 데 도움이 된다.\n",
    "\n",
    "\n",
    "4. **length_function**:\n",
    "   - **설명**: 청크의 길이를 계산하는 함수이다. 일반적으로 문자열 길이를 계산하는 `len` 함수를 사용한다.\n",
    "\n",
    "\n",
    "5. **is_separator_regex**:\n",
    "   - **설명**: 구분자가 정규 표현식인지 여부를 설정한다. `False`로 설정하면 일반 문자열로 처리된다.\n",
    "\n",
    "\n",
    "## 사용 목적\n",
    "이 과정을 통해 대량의 텍스트를 효과적으로 처리하고, AI 모델이 이해할 수 있는 형태로 변환하여 정보 추출 및 분석을 용이하게 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': '인공지능산업최신동향_2024년11월호.pdf', 'page': 0}, page_content='2024년 11월호')]\n"
     ]
    }
   ],
   "source": [
    "#4. 문서 청크로 나누기(RecursiveCharacterTextSplitter)\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# # 텍스트 분할기 설정 (재귀적 문자 기반 분할)\n",
    "recursive_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=10,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "splits = recursive_text_splitter.split_documents(docs) # 문서를 청크로 분할\n",
    "print(splits[:1]) # 상위 10개만 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RecursiveCharacterTextSplitter 청킹 방식 및 매개변수 설명\n",
    "\n",
    "## 청킹 방식\n",
    "`RecursiveCharacterTextSplitter`는 텍스트를 재귀적으로 나누어 작은 조각(청크)으로 만드는 기능을 제공한다. 이 방식은 긴 문서나 복잡한 내용을 효과적으로 처리할 수 있다.\n",
    "\n",
    "## 매개변수 설명\n",
    "\n",
    "1. **chunk_size**:\n",
    "   - **설명**: 각 청크의 최대 크기를 설정한다. 청크는 이 크기를 초과하지 않는다.\n",
    "\n",
    "2. **chunk_overlap**:\n",
    "   - **설명**: 청크 간의 겹치는 글자 수를 설정한다. 문맥을 유지하는 데 도움이 된다.\n",
    "\n",
    "3. **length_function**:\n",
    "   - **설명**: 청크의 길이를 계산하는 함수이다. 일반적으로 문자열 길이를 계산하는 `len` 함수를 사용한다.\n",
    "\n",
    "4. **is_separator_regex**:\n",
    "   - **설명**: 구분자가 정규 표현식인지 여부를 설정한다. `False`로 설정하면 일반 문자열로 처리된다.\n",
    "\n",
    "## 사용 목적\n",
    "이 과정을 통해 대량의 텍스트를 효과적으로 처리하고, AI 모델이 이해할 수 있는 형태로 변환하여 정보 추출 및 분석을 용이하게 한다. `RecursiveCharacterTextSplitter`는 문맥을 잘 유지하면서 청크를 나누기 때문에, 복잡한 텍스트에 특히 유용하다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5 벡터 임베딩 생성\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# OpenAI 임베딩 모델 초기화\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6. 벡터 스토어 생성\n",
    "import faiss\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# 문서에서 벡터 저장소 생성\n",
    "vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7. FAISS를 Retriever로 변환\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#8. 프롬프트 템플릿을 정의하라\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# 프롬프트 템플릿 정의\n",
    "contextual_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Answer the question using only the following context.\"),\n",
    "    (\"user\", \"Context: {context}\\\\n\\\\nQuestion: {question}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#9. LangChain의 모델과 프롬프트를 연결하여 RAG 체인을 구성하라. \n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "class SimplePassThrough:\n",
    "    def invoke(self, inputs, **kwargs):\n",
    "        return inputs\n",
    "\n",
    "class ContextToPrompt:\n",
    "    def __init__(self, prompt_template):\n",
    "        self.prompt_template = prompt_template\n",
    "    \n",
    "    def invoke(self, inputs):\n",
    "        # 문서 내용을 텍스트로 변환\n",
    "        if isinstance(inputs, list):\n",
    "            context_text = \"\\n\".join([doc.page_content for doc in inputs])\n",
    "        else:\n",
    "            context_text = inputs\n",
    "        \n",
    "        # 프롬프트 템플릿에 적용\n",
    "        formatted_prompt = self.prompt_template.format_messages(\n",
    "            context=context_text,\n",
    "            question=inputs.get(\"question\", \"\")\n",
    "        )\n",
    "        return formatted_prompt\n",
    "\n",
    "# Retriever를 invoke() 메서드로 래핑하는 클래스 정의\n",
    "class RetrieverWrapper:\n",
    "    def __init__(self, retriever):\n",
    "        self.retriever = retriever\n",
    "\n",
    "    def invoke(self, inputs):\n",
    "        if isinstance(inputs, dict):\n",
    "            query = inputs.get(\"question\", \"\")\n",
    "        else:\n",
    "            query = inputs\n",
    "        # 검색 수행\n",
    "        response_docs = self.retriever.get_relevant_documents(query)\n",
    "        return response_docs\n",
    "#텍스트 생성 체인 생성\n",
    "llm_chain = LLMChain(llm=model, prompt=contextual_prompt)\n",
    "\n",
    "# RAG 체인 설정\n",
    "rag_chain_debug = {\n",
    "    \"context\": RetrieverWrapper(retriever),\n",
    "    \"prompt\": ContextToPrompt(contextual_prompt),\n",
    "    \"llm\": model\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG(생성형 응답 생성)의 필요성\n",
    "\n",
    "RAG는 정보 검색과 생성 모델을 결합하여 사용자 질문에 대한 보다 정확하고 관련성 높은 답변을 생성하는 기술이다. RAG의 필요성은 다음과 같은 이유로 설명할 수 있다.\n",
    "\n",
    "1. **정보의 동적 접근**: RAG는 최신 정보를 실시간으로 검색할 수 있어, 사용자가 요청하는 질문에 대해 항상 최신의 정확한 정보를 제공할 수 있다.\n",
    "\n",
    "2. **맥락 이해**: 사용자 질문에 대한 답변을 생성할 때, RAG는 관련된 맥락을 기반으로 하여 보다 깊이 있는 이해를 제공합니다. 이는 단순한 정답 제공을 넘어, 질문의 의도를 파악하여 적절한 답변을 생성하는 데 기여한다.\n",
    "\n",
    "3. **대량의 데이터 처리**: RAG는 대량의 문서와 정보를 처리할 수 있는 능력을 가지고 있어, 사용자가 원하는 다양한 주제에 대해 신속하게 대응할 수 있다.\n",
    "\n",
    "4. **개선된 사용자 경험**: RAG를 통해 제공되는 답변은 보다 개인화되고 관련성이 높아, 사용자 경험을 개선할 수 있다. 이는 사용자 만족도를 높이고, 반복적인 질문을 줄이는 데 도움이 된다.\n",
    "\n",
    "결론적으로, RAG는 정보 검색과 생성 모델의 장점을 결합하여, 더 나은 품질의 질문 응답 시스템을 구축하는 데 필수적이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================\n",
      "\n",
      "답변:\n",
      "인공지능은 컴퓨터 시스템이 인간의 지능을 모방하여 학습, 추론, 문제 해결 등의 작업을 수행할 수 있도록 하는 기술입니다.\n",
      "========================\n",
      "\n",
      "답변:\n",
      "문서는 공공 부문의 AI 도입을 안내하는 보고서로, AI 도입 시 단계별 접근방식을 권고합니다. 이 과정은 프로토타입부터 시작해 시범 도입을 거쳐 본격적으로 구현하는 방식으로 구성되어 있습니다.\n",
      "========================\n",
      "\n",
      "답변:\n",
      "저는 정보를 제공하고 질문에 답변하는 AI입니다.\n",
      "========================\n"
     ]
    }
   ],
   "source": [
    "#10. 질문에 응답하는 챗봇을 구동하여 질문해라. \n",
    "\n",
    "# 챗봇 구동\n",
    "while True:\n",
    "    print(\"========================\")\n",
    "    query = input(\"질문을 입력하세요 : \")\n",
    "    if query == \"종료\": # 종료 입력 시 챗봇 종료\n",
    "        break\n",
    "    \n",
    "    # 1. Retriever로 관련 문서 검색\n",
    "    response_docs = rag_chain_debug[\"context\"].invoke({\"question\": query})\n",
    "    \n",
    "    # 2. 문서를 프롬프트로 변환\n",
    "    prompt_messages = rag_chain_debug[\"prompt\"].invoke({\n",
    "        \"context\": response_docs,\n",
    "        \"question\": query\n",
    "    })\n",
    "    \n",
    "    # 3. LLM으로 응답 생성\n",
    "    response = rag_chain_debug[\"llm\"].invoke(prompt_messages)\n",
    "    \n",
    "    print(\"\\n답변:\")\n",
    "    print(response.content) # 답변 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\h\\Desktop\\TIL\\.venv\\Lib\\site-packages\\langsmith\\client.py:241: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n",
      "c:\\Users\\h\\Desktop\\TIL\\.venv\\Lib\\site-packages\\langsmith\\client.py:241: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n",
      "c:\\Users\\h\\Desktop\\TIL\\.venv\\Lib\\site-packages\\langsmith\\client.py:241: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "폴더 및 파일이 성공적으로 생성되었습니다.\n"
     ]
    }
   ],
   "source": [
    "#도전 과제 1. LangSmith의 Prompt Library 를 참고하여 프롬프트를 3개 이상 아래와 같은 파일 구조로 저장해라. \n",
    "\n",
    "import os\n",
    "from langchain import hub\n",
    "\n",
    "# 폴더 및 파일 이름 설정\n",
    "folder_name = 'Prompts'\n",
    "file_names = ['prompt1.txt', 'prompt2.txt', 'prompt3.txt']\n",
    "\n",
    "# Prompts 폴더 생성\n",
    "os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "# 외부 저장소에서 프롬프트를 가져옴\n",
    "prompt1 = hub.pull(\"jisujiji/rag-prompt-1\") # 첫 번째 프롬프트\n",
    "prompt2 = hub.pull(\"jakab/rag-prompt\") # 두 번째 프롬프트\n",
    "prompt3 = hub.pull(\"rlm/rag-answer-hallucination\") # 세 번째 프롬프트\n",
    "\n",
    "\n",
    "# 프롬프트 파일 내용 작성\n",
    "prompts = [\n",
    "    str(prompt1), # 첫 번째 파일 내용\n",
    "    str(prompt2), # 두 번째 파일 내용\n",
    "    str(prompt3) # 세 번째 파일 내용\n",
    "]\n",
    "\n",
    "# 프롬프트 파일 생성\n",
    "for file_name, prompt in zip(file_names, prompts):\n",
    "    with open(os.path.join(folder_name, file_name), 'w', encoding='utf-8') as f:\n",
    "        f.write(prompt)\n",
    "\n",
    "print(\"폴더 및 파일이 성공적으로 생성되었습니다.\") # 결과 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================\n",
      "\n",
      "답변:\n",
      "인공지능은 인간의 지능을 모방하여 학습, 문제 해결, 의사 결정 등의 작업을 수행할 수 있는 시스템이나 프로그램을 의미합니다.\n",
      "========================\n",
      "\n",
      "답변:\n",
      "문서 내용은 카카오가 카나나라는 앱을 카카오톡과 별개로 출시할 계획에 대한 정보와 함께, 이용자를 지원하기 위해 스터디 그룹 대화에서 함께 읽은 논문 관련 퀴즈를 제공하고 채점 및 부연 설명을 하는 기능에 대한 설명이 포함되어 있습니다.\n",
      "========================\n",
      "\n",
      "답변:\n",
      "'인공지능산업최신동향_2024년11월호.pdf'라는 문서를 참고했습니다.\n",
      "========================\n",
      "\n",
      "답변:\n",
      "저는 로제타폴드의 핵심개발자이자 제1저자인 백민경 교수와 허사비스입니다.\n",
      "========================\n"
     ]
    }
   ],
   "source": [
    "#도전 과제 2. 각 프롬프트를 외부에서 불러와서 실행할 수 있도록 코드를 고쳐라. \n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "import os\n",
    "import time\n",
    "\n",
    "# 파일로 저장한 프롬프트를 가져옴\n",
    "while True:\n",
    "    try:\n",
    "        a = input(\"가져올 프롬프트 이름: \") \n",
    "        file_path = f\"C:\\\\Users\\\\h\\\\Desktop\\\\TIL\\\\personal_assignment\\\\Prompts\\\\{a}\" # 파일 경로\n",
    "        with open(file_path, \"r\") as f:\n",
    "            prompt = f.read() # 파일을 읽어서 저장\n",
    "        break\n",
    "    except FileNotFoundError and OSError:\n",
    "        print(\"없는 파일입니다!\")\n",
    "\n",
    "# 프롬프트 템플릿 생성\n",
    "prompt_template = ChatPromptTemplate.from_template(prompt)\n",
    "#텍스트 생성 체인 설정\n",
    "llm_chain = LLMChain(llm=model, prompt=prompt_template)\n",
    "\n",
    "\n",
    "#챗봇 구동\n",
    "while True:\n",
    "    \n",
    "    query = input(\"질문을 입력하세요 : \")\n",
    "    if query == \"종료\": # 종료 입력 시 챗봇 종료\n",
    "        break\n",
    "    \n",
    "    # 1. Retriever로 관련 문서 검색\n",
    "    response_docs = rag_chain_debug[\"context\"].invoke({\"question\": query})\n",
    "    \n",
    "    # 2. 문서를 프롬프트로 변환\n",
    "    prompt_messages = rag_chain_debug[\"prompt\"].invoke({\n",
    "        \"context\": response_docs,\n",
    "        \"question\": query\n",
    "    })\n",
    "    \n",
    "    # 3. LLM으로 응답 생성\n",
    "    response = rag_chain_debug[\"llm\"].invoke(prompt_messages)\n",
    "    \n",
    "    print(\"\\n답변:\")\n",
    "    print(response.content) # 답변 출력\n",
    "\n",
    "    # 도전 과제 3번 실행 결과는 자동으로 Result 디렉토리에 저장되어야 한다. 이때, 실험 결과 파일 이름은 실험에 쓰인 프롬프트의 이름과 timestamp을 포함해야한다.\n",
    "    \n",
    "    # 현재 타임스탬프 기록\n",
    "    timestamp = time.time()\n",
    "\n",
    "    # 폴더 및 파일 이름 설정\n",
    "    folder_name = 'Results'\n",
    "    file_name = f\"{a[:7]}_result_{timestamp}.txt\"\n",
    "\n",
    "    # 폴더 생성\n",
    "    os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "    # 답변 파일 생성\n",
    "    with open(os.path.join(folder_name, file_name), 'w', encoding='utf-8') as f:\n",
    "        f.write(response.content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
